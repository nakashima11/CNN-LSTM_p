{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdVlBI8v2oYx",
        "outputId": "a472d710-8fa9-474f-90dc-0afe0021cd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "\n",
        "data_folder = \"/content/drive/Othercomputers/マイ iMac/2024_中島/visual studio/波形/mnt/data/extend_data\"\n",
        "label_file = \"/content/drive/Othercomputers/マイ iMac/2024_中島/visual studio/波形/mnt/data/label_data/label_data(ex).csv\"\n",
        "\n",
        "label_df = pd.read_csv(label_file)\n",
        "#データの最大値、最小値の取得\n",
        "global_min = {\"x_data\": float(\"inf\"), \"y_data\": float(\"inf\"), \"x_data_fft\": float(\"inf\"), \"y_data_fft\": float(\"inf\")}\n",
        "global_max = {\"x_data\": float(\"-inf\"), \"y_data\": float(\"-inf\"), \"x_data_fft\": float(\"-inf\"), \"y_data_fft\": float(\"-inf\")}\n",
        "\n",
        "for _, row in label_df.iterrows():\n",
        "    file_name = row[\"file_name\"]\n",
        "    file_path = os.path.join(data_folder, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        for key in global_min.keys():\n",
        "            global_min[key] = min(global_min[key], df[key].min())\n",
        "            global_max[key] = max(global_max[key], df[key].max())\n",
        "\n",
        "print(\"取得した最大・最小値\")\n",
        "print(global_min)\n",
        "print(global_max)\n"
      ],
      "metadata": {
        "id": "wUBy1TVt3ajx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7927b9c5-fa55-4f31-b9e4-2050c165a522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "取得した最大・最小値\n",
            "{'x_data': -0.15785985886834, 'y_data': -0.2151365352777189, 'x_data_fft': 0.0, 'y_data_fft': 0.0}\n",
            "{'x_data': 0.1554234849246505, 'y_data': 0.2118111809023308, 'x_data_fft': 33.977, 'y_data_fft': 40.005}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#データの正規化\n",
        "def normalize(data, min_val, max_val):\n",
        "    return (data - min_val) / (max_val - min_val)\n",
        "\n",
        "all_data = []\n",
        "all_labels = []\n",
        "\n",
        "for _, row in label_df.iterrows():\n",
        "    file_name, label = row[\"file_name\"], row[\"label\"]\n",
        "    file_path = os.path.join(data_folder, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        x_data_norm = normalize(df[\"x_data\"].values, global_min[\"x_data\"], global_max[\"x_data\"])\n",
        "        y_data_norm = normalize(df[\"y_data\"].values, global_min[\"y_data\"], global_max[\"y_data\"])\n",
        "        x_data_fft_norm = normalize(df[\"x_data_fft\"].values, global_min[\"x_data_fft\"], global_max[\"x_data_fft\"])\n",
        "        y_data_fft_norm = normalize(df[\"y_data_fft\"].values, global_min[\"y_data_fft\"], global_max[\"y_data_fft\"])\n",
        "\n",
        "        normalized_data = np.stack([x_data_norm, y_data_norm, x_data_fft_norm, y_data_fft_norm], axis=0)\n",
        "        all_data.append(normalized_data)\n",
        "        all_labels.append(label)\n",
        "\n",
        "X = np.array(all_data)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "print(\" 正規化後のデータ形状:\", X.shape)\n",
        "print(\" ラベルの形状:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghaZC1K-CggH",
        "outputId": "c7044e2a-e133-465c-ee3f-fdbe660086d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 正規化後のデータ形状: (0,)\n",
            " ラベルの形状: (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  データの分割 (80% を訓練, 20% をテスト)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u1icgzxv3qhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-LSTMモデルの定義\n",
        "class CNN_LSTM_Waveform(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_LSTM_Waveform, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(4, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=256, num_layers=3, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = out.permute(0, 2, 1)\n",
        "        lstm_out, _ = self.lstm(out)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # 全結合層\n",
        "        out = self.fc1(lstm_out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "40HgmCfv3wG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-vGB6l4UAcBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#学習\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_LSTM_Waveform().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "def train_model():\n",
        "    model.train()\n",
        "    num_epochs = 30\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # 順伝播\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            # 逆伝播と最適化\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        loss_history.append(epoch_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    plt.plot(loss_history, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "dQV3NDUr34F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#学習\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batch_size = test_loader.batch_size\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"学習データ数: {len(train_dataset)}\")\n",
        "    print(f\"未学習データ数: {len(test_dataset)}\")\n",
        "    print(f\"バッチサイズ: {batch_size}\")\n",
        "    print(f\"バッチ数: {num_batches}\")\n",
        "    print(f\":Accurcy {correct}/{total} ({accuracy:.2f}%)\")\n",
        "\n",
        "\n",
        "train_model()\n",
        "evaluate_model()\n"
      ],
      "metadata": {
        "id": "NJ2wDQFA4gOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"cnn_lstm.pth\")\n"
      ],
      "metadata": {
        "id": "Xp-iZTak6iaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"cnn_lstm.pth\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "UqFD0k4v6j-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CNN_LSTM_Waveform(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_LSTM_Waveform, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=256, num_layers=3, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = out.permute(0, 2, 1)\n",
        "        lstm_out, _ = self.lstm(out)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        out = self.fc1(lstm_out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = \"/mnt/data/trained_model.pth\"\n",
        "model = CNN_LSTM_Waveform().to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "ZxdAmnBTSo-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}